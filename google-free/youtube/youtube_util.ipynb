{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlaylistItem(object):\n",
    "    def __init__(self, title, updated_at, url):\n",
    "        self.title = title\n",
    "        self.updated_at = updated_at\n",
    "        self.url = url\n",
    "        \n",
    "    def marshall(self):\n",
    "        d = {\n",
    "            'title': self.title,\n",
    "            'updated_at': self.updated_at,\n",
    "            'url': self.url\n",
    "        }\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin, urlparse, urlencode, ParseResult\n",
    "\n",
    "\n",
    "BASE_URL = 'https://www.youtube.com'\n",
    "WATCH_SUB_DOMAIN = 'watch'\n",
    "\n",
    "\n",
    "# Construct the YouTube video location given the extracted video id\n",
    "# https://www.youtube.com/watch?v={video_id}\n",
    "def video_url_from_id(video_id):\n",
    "    joined = urljoin(BASE_URL, WATCH_SUB_DOMAIN)\n",
    "    parsed = urlparse(joined)\n",
    "\n",
    "    pr = \\\n",
    "        ParseResult(\n",
    "            scheme=parsed.scheme,\n",
    "            netloc=parsed.netloc,\n",
    "            path=parsed.path,\n",
    "            params=None,\n",
    "            query=urlencode({'v': video_id}),\n",
    "            fragment=parsed.fragment\n",
    "        )\n",
    "\n",
    "    return pr.geturl()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_playlist_ids_from_response(resp):\n",
    "    collect = []\n",
    "    for i in resp['items']:\n",
    "        collect.append(i['id'])\n",
    "    \n",
    "    return collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests as req\n",
    "from typing import List\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import googleapiclient.discovery\n",
    "\n",
    "SERVICE = 'youtube'\n",
    "API_VERSION = 'v3'\n",
    "\n",
    "CHANNEL_ID = os.environ['CHANNEL_ID']\n",
    "API_KEY = os.environ['API_KEY']\n",
    "\n",
    "\n",
    "# Return an authenticated service session to be used in future requests.\n",
    "# E.g., `youtube_session = get_authenticated_service()`\n",
    "# To validate a session an API Key needs to be created as per: \n",
    "# https://developers.google.com/youtube/v3/getting-started\n",
    "def get_authenticated_service():\n",
    "    return googleapiclient.discovery.build(\n",
    "        SERVICE, API_VERSION, developerKey=API_KEY\n",
    "    )\n",
    "\n",
    "\n",
    "# only public playlists (excludes unlisted and private) belonging to\n",
    "# my account are listed\n",
    "def retrieve_playlist_ids(youtube):\n",
    "    playlist_ids = []\n",
    "    next_page = None\n",
    "    make_request = True\n",
    "    \n",
    "    while make_request:\n",
    "        req = youtube.playlists().list(\n",
    "            part='snippet',\n",
    "            channelId=CHANNEL_ID,\n",
    "            pageToken = next_page,\n",
    "            maxResults=50\n",
    "        )\n",
    "        resp = req.execute()\n",
    "        playlist_ids = playlist_ids + _extract_playlist_ids_from_response(resp)\n",
    "    \n",
    "        try:\n",
    "            next_page = resp['nextPageToken']\n",
    "        except KeyError:\n",
    "            make_request = False \n",
    "\n",
    "    return playlist_ids\n",
    "    \n",
    "\n",
    "def dump_channel_playlist_to_file():\n",
    "    youtube_session = get_authenticated_service()\n",
    "    response = retrieve_playlists(youtube_session)\n",
    "    with open('playlists.json', 'w') as fp:\n",
    "        js = json.dumps(response, sort_keys=True, indent=2)\n",
    "        print(js, file=fp)\n",
    "\n",
    "\n",
    "# The extracted id (from the playlist dump) is equalivalent to the\n",
    "# playlist url id in the form: youtube.com/watch?v={video}&list={playlist_id}\n",
    "def dump_from_playlist(youtube, playlist_id):\n",
    "    req = youtube.playlistItems().list(\n",
    "        part='snippet',\n",
    "        playlistId=playlist_id,\n",
    "        maxResults=50\n",
    "    )\n",
    "    return req.execute()\n",
    "\n",
    "\n",
    "# Extract selected data from an API request\n",
    "def _extract_playlist_items_from_response(response) -> List[PlaylistItem]:\n",
    "    collect = []\n",
    "    for i in response['items']:\n",
    "        snippet = i['snippet']\n",
    "        title = snippet['title']\n",
    "        # description = snippet['description']\n",
    "        time = snippet['publishedAt']\n",
    "        video_id = snippet['resourceId']['videoId']\n",
    "        url = video_url_from_id(video_id)\n",
    "        playlist_item = PlaylistItem(title, time, url)\n",
    "        collect.append(playlist_item)\n",
    "    return collect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the playlist's name from the playlist id\n",
    "def playlist_name_from_id(youtube, playlist_id):\n",
    "    req = youtube.playlists().list(\n",
    "        part='snippet',\n",
    "        id=playlist_id\n",
    "    )\n",
    "    \n",
    "    resp = req.execute()\n",
    "    \n",
    "    if len(resp['items']) < 1:\n",
    "        raise UnboundLocalError\n",
    "    else:\n",
    "        return resp['items'][0]['snippet']['localized']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all the items from a playlist and render in 2-tuple format:\n",
    "# (playlist_name, PlaylistItem objects)\n",
    "def extract_all_from_playlist(youtube, playlist_id):\n",
    "    \n",
    "    playlist_items = []\n",
    "    next_page = None\n",
    "    make_request = True\n",
    "    playlist_name = playlist_name_from_id(youtube, playlist_id)\n",
    "    \n",
    "    while make_request:\n",
    "        req = youtube.playlistItems().list(\n",
    "            part='snippet',\n",
    "            playlistId=playlist_id,\n",
    "            # if the number of items in the playlist > maxResults, there will be a\n",
    "            # `nextPageToken` in the snippet, pass the `nextPageToken` as the `pageToken` param\n",
    "            pageToken=next_page,\n",
    "            maxResults=50\n",
    "        )\n",
    "        \n",
    "        resp = req.execute()\n",
    "        playlist_items = playlist_items + _extract_playlist_items_from_response(resp)\n",
    "        \n",
    "        try:\n",
    "            next_page = resp['nextPageToken']\n",
    "        except KeyError:\n",
    "            make_request = False\n",
    "    \n",
    "    return (playlist_name, playlist_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelItem(object):\n",
    "    def __init__(self, channel, published_at, url):\n",
    "        self.channel = channel\n",
    "        self.published_at = published_at\n",
    "        self.url = url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "CHANNEL_SUB_DOMAIN = 'channel'\n",
    "\n",
    "# Construct the YouTube channel location given the extracted channel id\n",
    "# https://www.youtube.com/channel/{channel_id}\n",
    "def channel_url_from_id(channel_id) -> str:\n",
    "    # Is there any danger in using os.path join rather than urllib's join?\n",
    "    full_path = join(BASE_URL, CHANNEL_SUB_DOMAIN, channel_id)\n",
    "    return full_path\n",
    "    \n",
    "\n",
    "def _load_from_file(file_path):\n",
    "    with open(file_path, 'r') as fp:\n",
    "        js = json.load(fp)\n",
    "        \n",
    "    return js\n",
    "    \n",
    "    \n",
    "# Load channels from json file to list of ChannelItem objects\n",
    "def load_channels_from_file_to_obj(file_path) -> List[ChannelItem]:\n",
    "    collect = []\n",
    "    with open(file_path, 'r') as fp:\n",
    "        js = json.load(fp)\n",
    "    \n",
    "    items = js['items']\n",
    "    for item in items:\n",
    "        snippet = item['snippet']\n",
    "        channel = snippet['title']\n",
    "        channel_id = snippet['resourceId']['channelId']\n",
    "        published_at = snippet['publishedAt']\n",
    "        url = channel_url_from_id(channel_id)\n",
    "        ci = ChannelItem(channel, published_at, url)\n",
    "        collect.append(ci)\n",
    "        \n",
    "    return collect\n",
    "    \n",
    "\n",
    "# Traverse all the files in a directory related to JSON data dump of subscription data\n",
    "# And return a merged list of all the subscriptions as ChannelItem objects\n",
    "def extract_channels_from_dir(file_path='subs/') -> List[ChannelItem]:\n",
    "    only_json = [f for f in listdir(file_path) if isfile(join(file_path, f)) and f.endswith('.json')]\n",
    "    channels = []\n",
    "    for f in only_json:\n",
    "        full_path = join(file_path, f)\n",
    "        channels = channels + load_channels_from_file_to_obj(full_path)\n",
    "    return channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_load_from_file('subs/1.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
